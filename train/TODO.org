* Codebase
** TODO Unify front/back-end with training code (merge branches)
** DONE Auto-call `mount_remotes_development.sh` and `cp -r /mnt/afs/chesapeake/demo_data/ data/` when VM re-started

* Instances
** TODO Run separate UI instance of back-prop fine-tuning to share with group

* UI
** TODO Merge from `dev` branch
*** Run Caleb's new data-gen script to make this work
** Re-train and re-predict as soon as each correction provided
Can this be done fast enough?
  Probably not right now...

** Current commands:
*** Back-end:  python backend_server.py --port 4444 --model nips_sr --model_fn /mnt/blobfuse/train-output/ForICCV/ForICCV-landcover-batch_size-16-loss-superres-lr-0.003-model-unet2-schedule-stepped-note-replication_1/final_model.h5 --gpu 0 --verbose
*** Front-end:  python frontend_server.py

* Fine-tuning methods
** TODO Ask Nebojsa about adversarial domain adaptation
** Gradient-descent fine-tuning
*** 1...n layers
*** LR = 0.003, 0.03, 0.3, ...
*** TODO In backend: ensure GPU use
*** TODO Use Adam
** TODO Set up train/fine-tune/val set --> comparison
*** Back-prop fine-tuning
*** Group-norm fine-tuning
*** Drop-out (+ last-layer)
** TODO Simulate user clicks
*** detect mistakes in fine-tuning region
*** train on some small # of them
** Auto-generate more data
*** polygons
*** flood fill
*** unsupervised clustering

* Studies

** Overall question:
*** How to solve practical domain adaptation problem w/ min human labor?
*** Hope: coupling human & AI, can see co-adaptation that's interesting in its own right

** Do practice session for Andi & Caleb

** Compare # labels & time required to reach certain accuracy
*** User-selected, random, entropy
*** User gets better over time?
*** Which fine-tuning method yields best efficiency w/ human in the loop?  
Different than which one works best w/o human in the loop?
*** Where do users make corrections? Which order? Does it cluster in particular areas?
*** How often do they re-check the same spot's predictions?


** Combine multiple user's corrections together
*** Does overall efficiency go down, because users lose track of their own impact?
** Swap one user's model with another user
*** Do they lose their "theory of mind" of the model?

* Other Approaches
** Hierarchical nearest neighbors
** Probabilistic programming: symbolic constraints on output
*** Use Picture? http://mrkulk.github.io/www_cvpr15/
*** Learn rules?
*** Mine-sweeper: Figure out where you need to ask for a label to get the most information (use that label to deduce other labels). 
** Imitation learning on human actions (when to zoom, pan, which sequence to label things)
*** Eg. zoom out, run prediction, zoom in, make up-sampled / super-resolution labels, condition zoomed-in labels off super-res ones, pan left, etc.
*** Follow deduction-order
** Architectures combining different zoom levels at once
** Architectures combining predictions from neighboring patches
